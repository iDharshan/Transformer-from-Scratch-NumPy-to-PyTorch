{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attention weights for 'I like Batman':\n",
      "I -> I: 0.766\n",
      "I -> like: 0.124\n",
      "I -> Batman: 0.110\n",
      "like -> I: 0.455\n",
      "like -> like: 0.160\n",
      "like -> Batman: 0.385\n",
      "Batman -> I: 0.334\n",
      "Batman -> like: 0.452\n",
      "Batman -> Batman: 0.214\n",
      "\n",
      "Attention weights for 'I like Dwight Schrute from The Office':\n",
      "I -> I: 0.357\n",
      "I -> like: 0.058\n",
      "I -> Dwight: 0.090\n",
      "I -> Schrute: 0.270\n",
      "I -> from: 0.101\n",
      "I -> The: 0.083\n",
      "I -> Office: 0.041\n",
      "like -> I: 0.103\n",
      "like -> like: 0.036\n",
      "like -> Dwight: 0.010\n",
      "like -> Schrute: 0.184\n",
      "like -> from: 0.430\n",
      "like -> The: 0.043\n",
      "like -> Office: 0.194\n",
      "Dwight -> I: 0.038\n",
      "Dwight -> like: 0.367\n",
      "Dwight -> Dwight: 0.075\n",
      "Dwight -> Schrute: 0.163\n",
      "Dwight -> from: 0.035\n",
      "Dwight -> The: 0.241\n",
      "Dwight -> Office: 0.081\n",
      "Schrute -> I: 0.575\n",
      "Schrute -> like: 0.022\n",
      "Schrute -> Dwight: 0.061\n",
      "Schrute -> Schrute: 0.065\n",
      "Schrute -> from: 0.095\n",
      "Schrute -> The: 0.050\n",
      "Schrute -> Office: 0.132\n",
      "from -> I: 0.176\n",
      "from -> like: 0.174\n",
      "from -> Dwight: 0.100\n",
      "from -> Schrute: 0.112\n",
      "from -> from: 0.061\n",
      "from -> The: 0.044\n",
      "from -> Office: 0.333\n",
      "The -> I: 0.271\n",
      "The -> like: 0.062\n",
      "The -> Dwight: 0.125\n",
      "The -> Schrute: 0.080\n",
      "The -> from: 0.071\n",
      "The -> The: 0.091\n",
      "The -> Office: 0.300\n",
      "Office -> I: 0.103\n",
      "Office -> like: 0.172\n",
      "Office -> Dwight: 0.163\n",
      "Office -> Schrute: 0.025\n",
      "Office -> from: 0.194\n",
      "Office -> The: 0.094\n",
      "Office -> Office: 0.249\n"
     ]
    }
   ],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.W_query = torch.randn(embedding_dim, embedding_dim)\n",
    "        self.W_key = torch.randn(embedding_dim, embedding_dim)\n",
    "        self.W_value = torch.randn(embedding_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Q = torch.matmul(X, self.W_query)\n",
    "        K = torch.matmul(X, self.W_key)\n",
    "        V = torch.matmul(X, self.W_value)\n",
    "\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "        attention_scores = attention_scores / torch.sqrt(torch.tensor(self.embedding_dim))\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        return output, attention_weights\n",
    "\n",
    "# Process sentences\n",
    "sentence1 = [\"I\", \"like\", \"Batman\"]\n",
    "sentence2 = [\"I\", \"like\", \"Dwight\", \"Schrute\", \"from\", \"The\", \"Office\"]\n",
    "\n",
    "# Combine unique words from both sentences\n",
    "unique_words = list(set(sentence1 + sentence2))\n",
    "vocab_size = len(unique_words)\n",
    "word_to_idx = {word: idx for idx, word in enumerate(unique_words)}\n",
    "\n",
    "# Create one-hot encodings\n",
    "def create_one_hot(sentence, word_to_idx, vocab_size):\n",
    "    encodings = torch.zeros((len(sentence), vocab_size))\n",
    "    for i, word in enumerate(sentence):\n",
    "        encodings[i][word_to_idx[word]] = 1\n",
    "    return encodings\n",
    "\n",
    "# Process each sentence separately\n",
    "X1 = create_one_hot(sentence1, word_to_idx, vocab_size)\n",
    "X2 = create_one_hot(sentence2, word_to_idx, vocab_size)\n",
    "\n",
    "# Initialize and apply self-attention\n",
    "attention = SelfAttention(embedding_dim=vocab_size)\n",
    "\n",
    "# Process sentence 1\n",
    "output1, attention_weights1 = attention.forward(X1)\n",
    "print(\"\\nAttention weights for 'I like Batman':\")\n",
    "for i, word1 in enumerate(sentence1):\n",
    "    for j, word2 in enumerate(sentence1):\n",
    "        print(f\"{word1} -> {word2}: {attention_weights1[i][j]:.3f}\")\n",
    "\n",
    "# Process sentence 2\n",
    "output2, attention_weights2 = attention.forward(X2)\n",
    "print(\"\\nAttention weights for 'I like Dwight Schrute from The Office':\")\n",
    "for i, word1 in enumerate(sentence2):\n",
    "    for j, word2 in enumerate(sentence2):\n",
    "        print(f\"{word1} -> {word2}: {attention_weights2[i][j]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, p=0.3):  # Increased dropout for small data\n",
    "        self.p = p\n",
    "    \n",
    "    def forward(self, x, training=True):\n",
    "        if not training or self.p == 0:\n",
    "            return x\n",
    "        mask = torch.bernoulli(torch.full_like(x, 1 - self.p)) / (1 - self.p)\n",
    "        return x * mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention:\n",
    "    def __init__(self, dmodel, num_heads, dropout=0.3, device=None):\n",
    "        self.dmodel = dmodel\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dmodel // num_heads\n",
    "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Xavier initialization\n",
    "        self.W_query = [nn.Parameter(torch.empty(dmodel, self.head_dim, device=self.device)) for _ in range(num_heads)]\n",
    "        self.W_key = [nn.Parameter(torch.empty(dmodel, self.head_dim, device=self.device)) for _ in range(num_heads)]\n",
    "        self.W_value = [nn.Parameter(torch.empty(dmodel, self.head_dim, device=self.device)) for _ in range(num_heads)]\n",
    "        self.W_output = nn.Parameter(torch.empty(self.head_dim * num_heads, dmodel, device=self.device))\n",
    "        \n",
    "        for w in self.W_query + self.W_key + self.W_value + [self.W_output]:\n",
    "            nn.init.xavier_uniform_(w, gain=1.0)\n",
    "        \n",
    "        self.dropout = Dropout(dropout)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_x = torch.exp(x - torch.max(x, dim=-1, keepdim=True)[0])\n",
    "        return exp_x / exp_x.sum(dim=-1, keepdim=True)\n",
    "    \n",
    "    def forward(self, X, memory=None, mask=None, training=True):\n",
    "        head_outputs = []\n",
    "        all_attention_weights = []\n",
    "        \n",
    "        for head in range(self.num_heads):\n",
    "            Q = torch.matmul(X, self.W_query[head])\n",
    "            K = torch.matmul(memory if memory is not None else X, self.W_key[head])\n",
    "            V = torch.matmul(memory if memory is not None else X, self.W_value[head])\n",
    "            \n",
    "            scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "            if mask is not None:\n",
    "                scores = scores.masked_fill(mask == 0, -1e9)\n",
    "            weights = self.softmax(scores)\n",
    "            output = torch.matmul(weights, V)\n",
    "            head_outputs.append(self.dropout.forward(output, training))\n",
    "            all_attention_weights.append(weights)\n",
    "        \n",
    "        multi_head_output = torch.cat(head_outputs, dim=-1)\n",
    "        final_output = torch.matmul(multi_head_output, self.W_output)\n",
    "        final_output = self.dropout.forward(final_output, training)\n",
    "        \n",
    "        return final_output, all_attention_weights\n",
    "    \n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        params.extend(self.W_query)\n",
    "        params.extend(self.W_key)\n",
    "        params.extend(self.W_value)\n",
    "        params.append(self.W_output)\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN:\n",
    "    def __init__(self, dmodel, d_ff, device=None):\n",
    "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.W1 = nn.Parameter(torch.empty(dmodel, d_ff, device=self.device))\n",
    "        self.W2 = nn.Parameter(torch.empty(d_ff, dmodel, device=self.device))\n",
    "        self.b1 = nn.Parameter(torch.zeros(d_ff, device=self.device))\n",
    "        self.b2 = nn.Parameter(torch.zeros(dmodel, device=self.device))\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.W1, gain=1.0)\n",
    "        nn.init.xavier_uniform_(self.W2, gain=1.0)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        Z1 = torch.matmul(X, self.W1) + self.b1\n",
    "        A1 = torch.maximum(torch.zeros_like(Z1), Z1)  # ReLU\n",
    "        Z2 = torch.matmul(A1, self.W2) + self.b2\n",
    "        return Z2\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.W1, self.W2, self.b1, self.b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding:\n",
    "    def __init__(self, dmodel, max_seq_length=5000, device=None):\n",
    "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.encoding = torch.zeros(max_seq_length, dmodel, device=self.device)\n",
    "        position = torch.arange(max_seq_length, device=self.device).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, dmodel, 2, device=self.device) * (-math.log(10000.0) / dmodel))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term[:dmodel//2])\n",
    "    \n",
    "    def forward(self, X):\n",
    "        seq_length = X.shape[1]\n",
    "        return X + self.encoding[:seq_length]\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, vocab_size, dmodel, device=None):\n",
    "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.embedding_matrix = nn.Parameter(torch.empty(vocab_size, dmodel, device=self.device))\n",
    "        nn.init.xavier_uniform_(self.embedding_matrix, gain=1.0)\n",
    "        self.dmodel = dmodel\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embedding_matrix[x] * (self.dmodel ** 0.5)\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.embedding_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization:\n",
    "    def __init__(self, dmodel, epsilon=1e-6, device=None):\n",
    "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.gamma = nn.Parameter(torch.ones(dmodel, device=self.device))\n",
    "        self.beta = nn.Parameter(torch.zeros(dmodel, device=self.device))\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        normalized = (x - mean) / torch.sqrt(var + self.epsilon)\n",
    "        return self.gamma * normalized + self.beta\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer:\n",
    "    def __init__(self, dmodel, num_heads, d_ff, dropout=0.3, device=None):\n",
    "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.mha = MultiHeadAttention(dmodel, num_heads, dropout, self.device)\n",
    "        self.ffn = FFN(dmodel, d_ff, self.device)\n",
    "        self.norm1 = LayerNormalization(dmodel, device=self.device)\n",
    "        self.norm2 = LayerNormalization(dmodel, device=self.device)\n",
    "        self.dropout = Dropout(dropout)\n",
    "    \n",
    "    def forward(self, X, training=True):\n",
    "        attn_output, attn_weights = self.mha.forward(X, training=training)\n",
    "        dropout1 = self.dropout.forward(attn_output, training)\n",
    "        out1 = self.norm1.forward(X + dropout1)\n",
    "        \n",
    "        ffn_output = self.ffn.forward(out1)\n",
    "        dropout2 = self.dropout.forward(ffn_output, training)\n",
    "        out2 = self.norm2.forward(out1 + dropout2)\n",
    "        \n",
    "        return out2, attn_weights\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.mha.parameters() + self.ffn.parameters() + self.norm1.parameters() + self.norm2.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    def __init__(self, num_layers, dmodel, num_heads, d_ff, device=None):\n",
    "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.layers = [EncoderLayer(dmodel, num_heads, d_ff, device=self.device) for _ in range(num_layers)]\n",
    "    \n",
    "    def forward(self, X, training=True):\n",
    "        attention_weights = []\n",
    "        output = X\n",
    "        for layer in self.layers:\n",
    "            output, weights = layer.forward(output, training)\n",
    "            attention_weights.append(weights)\n",
    "        return output, attention_weights\n",
    "    \n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params.extend(layer.parameters())\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer:\n",
    "    def __init__(self, dmodel, num_heads, d_ff, dropout=0.3, device=None):\n",
    "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.masked_mha = MultiHeadAttention(dmodel, num_heads, dropout, self.device)\n",
    "        self.mha = MultiHeadAttention(dmodel, num_heads, dropout, self.device)\n",
    "        self.ffn = FFN(dmodel, d_ff, self.device)\n",
    "        self.norm1 = LayerNormalization(dmodel, device=self.device)\n",
    "        self.norm2 = LayerNormalization(dmodel, device=self.device)\n",
    "        self.norm3 = LayerNormalization(dmodel, device=self.device)\n",
    "        self.dropout = Dropout(dropout)\n",
    "    \n",
    "    def create_mask(self, size):\n",
    "        return torch.tril(torch.ones(size, size, device=self.device))\n",
    "    \n",
    "    def forward(self, X, encoded_output, training=True):\n",
    "        seq_length = X.shape[1]\n",
    "        mask = self.create_mask(seq_length)\n",
    "        \n",
    "        masked_attn_output, masked_weights = self.masked_mha.forward(X, mask=mask, training=training)\n",
    "        dropout1 = self.dropout.forward(masked_attn_output, training)\n",
    "        out1 = self.norm1.forward(X + dropout1)\n",
    "        \n",
    "        attn_output, attn_weights = self.mha.forward(out1, memory=encoded_output, training=training)\n",
    "        dropout2 = self.dropout.forward(attn_output, training)\n",
    "        out2 = self.norm2.forward(out1 + dropout2)\n",
    "        \n",
    "        ffn_output = self.ffn.forward(out2)\n",
    "        dropout3 = self.dropout.forward(ffn_output, training)\n",
    "        out3 = self.norm3.forward(out2 + dropout3)\n",
    "        \n",
    "        return out3, (masked_weights, attn_weights)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return (self.masked_mha.parameters() + self.mha.parameters() + \n",
    "                self.ffn.parameters() + self.norm1.parameters() + \n",
    "                self.norm2.parameters() + self.norm3.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder:\n",
    "    def __init__(self, num_layers, dmodel, num_heads, d_ff, device=None):\n",
    "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.layers = [DecoderLayer(dmodel, num_heads, d_ff, device=self.device) for _ in range(num_layers)]\n",
    "    \n",
    "    def forward(self, X, encoded_output, training=True):\n",
    "        attention_weights = []\n",
    "        output = X\n",
    "        for layer in self.layers:\n",
    "            output, weights = layer.forward(output, encoded_output, training)\n",
    "            attention_weights.append(weights)\n",
    "        return output, attention_weights\n",
    "    \n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params.extend(layer.parameters())\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 2.8731, LR: 0.000031\n",
      "Epoch 2/50, Loss: 2.3279, LR: 0.000063\n",
      "Epoch 3/50, Loss: 2.5458, LR: 0.000094\n",
      "Epoch 4/50, Loss: 2.6139, LR: 0.000125\n",
      "Epoch 5/50, Loss: 2.4611, LR: 0.000156\n",
      "Epoch 6/50, Loss: 2.5144, LR: 0.000188\n",
      "Epoch 7/50, Loss: 2.6135, LR: 0.000219\n",
      "Epoch 8/50, Loss: 2.4996, LR: 0.000250\n",
      "Epoch 9/50, Loss: 2.3353, LR: 0.000281\n",
      "Epoch 10/50, Loss: 1.9717, LR: 0.000313\n",
      "Epoch 11/50, Loss: 2.2963, LR: 0.000344\n",
      "Epoch 12/50, Loss: 2.5468, LR: 0.000375\n",
      "Epoch 13/50, Loss: 2.2445, LR: 0.000406\n",
      "Epoch 14/50, Loss: 2.0813, LR: 0.000438\n",
      "Epoch 15/50, Loss: 2.1527, LR: 0.000469\n",
      "Epoch 16/50, Loss: 2.2111, LR: 0.000500\n",
      "Epoch 17/50, Loss: 2.1250, LR: 0.000531\n",
      "Epoch 18/50, Loss: 2.2225, LR: 0.000563\n",
      "Epoch 19/50, Loss: 1.9191, LR: 0.000594\n",
      "Epoch 20/50, Loss: 2.4152, LR: 0.000625\n",
      "Epoch 21/50, Loss: 2.0863, LR: 0.000656\n",
      "Epoch 22/50, Loss: 2.2209, LR: 0.000688\n",
      "Epoch 23/50, Loss: 1.8868, LR: 0.000719\n",
      "Epoch 24/50, Loss: 2.1686, LR: 0.000750\n",
      "Epoch 25/50, Loss: 1.9884, LR: 0.000781\n",
      "Epoch 26/50, Loss: 1.8043, LR: 0.000813\n",
      "Epoch 27/50, Loss: 1.9388, LR: 0.000844\n",
      "Epoch 28/50, Loss: 2.1378, LR: 0.000875\n",
      "Epoch 29/50, Loss: 2.0887, LR: 0.000906\n",
      "Epoch 30/50, Loss: 2.1429, LR: 0.000938\n",
      "Epoch 31/50, Loss: 2.2873, LR: 0.000969\n",
      "Epoch 32/50, Loss: 2.2504, LR: 0.001000\n",
      "Epoch 33/50, Loss: 1.8794, LR: 0.001031\n",
      "Epoch 34/50, Loss: 2.1353, LR: 0.001063\n",
      "Epoch 35/50, Loss: 2.1087, LR: 0.001094\n",
      "Epoch 36/50, Loss: 2.0287, LR: 0.001125\n",
      "Epoch 37/50, Loss: 1.9107, LR: 0.001156\n",
      "Epoch 38/50, Loss: 1.8745, LR: 0.001187\n",
      "Epoch 39/50, Loss: 1.9603, LR: 0.001219\n",
      "Epoch 40/50, Loss: 1.8614, LR: 0.001250\n",
      "Epoch 41/50, Loss: 1.7957, LR: 0.001281\n",
      "Epoch 42/50, Loss: 1.9276, LR: 0.001313\n",
      "Epoch 43/50, Loss: 1.8323, LR: 0.001344\n",
      "Epoch 44/50, Loss: 1.9663, LR: 0.001375\n",
      "Epoch 45/50, Loss: 1.9072, LR: 0.001406\n",
      "Epoch 46/50, Loss: 1.7271, LR: 0.001438\n",
      "Epoch 47/50, Loss: 1.8073, LR: 0.001469\n",
      "Epoch 48/50, Loss: 1.6996, LR: 0.001500\n",
      "Epoch 49/50, Loss: 1.7683, LR: 0.001531\n",
      "Epoch 50/50, Loss: 1.6019, LR: 0.001563\n",
      "\n",
      "Input: I like to read\n",
      "Predicted translation: <start> j aime lire <end>\n"
     ]
    }
   ],
   "source": [
    "class Transformer:\n",
    "    def __init__(self, num_layers, dmodel, num_heads, d_ff, input_vocab_size, target_vocab_size, dropout=0.3, device=None):\n",
    "        self.dmodel = dmodel\n",
    "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.input_embedding = Embedding(input_vocab_size, dmodel, self.device)\n",
    "        self.output_embedding = Embedding(target_vocab_size, dmodel, self.device)\n",
    "        self.positional_encoding = PositionalEncoding(dmodel, device=self.device)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.encoder = Encoder(num_layers, dmodel, num_heads, d_ff, self.device)\n",
    "        self.decoder = Decoder(num_layers, dmodel, num_heads, d_ff, self.device)\n",
    "        self.final_layer = nn.Parameter(torch.empty(dmodel, target_vocab_size, device=self.device))\n",
    "        nn.init.xavier_uniform_(self.final_layer, gain=1.0)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_x = torch.exp(x - torch.max(x, dim=-1, keepdim=True)[0])\n",
    "        return exp_x / exp_x.sum(dim=-1, keepdim=True)\n",
    "    \n",
    "    def forward(self, input_seq, target_seq, training=True):\n",
    "        input_embedded = self.input_embedding.forward(input_seq)\n",
    "        input_encoded = self.positional_encoding.forward(input_embedded)\n",
    "        input_dropped = self.dropout.forward(input_encoded, training)\n",
    "        \n",
    "        encoded_output, encoder_attention = self.encoder.forward(input_dropped, training)\n",
    "        \n",
    "        target_embedded = self.output_embedding.forward(target_seq)\n",
    "        target_encoded = self.positional_encoding.forward(target_embedded)\n",
    "        target_dropped = self.dropout.forward(target_encoded, training)\n",
    "        \n",
    "        decoder_output, decoder_attention = self.decoder.forward(target_dropped, encoded_output, training)\n",
    "        \n",
    "        logits = torch.matmul(decoder_output, self.final_layer)\n",
    "        probs = self.softmax(logits)\n",
    "        \n",
    "        return probs, (encoder_attention, decoder_attention)\n",
    "    \n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        params.extend(self.input_embedding.parameters())\n",
    "        params.extend(self.output_embedding.parameters())\n",
    "        params.extend(self.positional_encoding.parameters())\n",
    "        params.extend(self.encoder.parameters())\n",
    "        params.extend(self.decoder.parameters())\n",
    "        params.append(self.final_layer)\n",
    "        return params\n",
    "\n",
    "    def translate(self, input_seq, max_length=20, start_token=1, end_token=2):\n",
    "        \"\"\"Autoregressive inference for translation.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Encode input\n",
    "            input_embedded = self.input_embedding.forward(input_seq)\n",
    "            input_encoded = self.positional_encoding.forward(input_embedded)\n",
    "            encoded_output, _ = self.encoder.forward(input_encoded, training=False)\n",
    "            \n",
    "            # Initialize target sequence with start token\n",
    "            target_seq = torch.full((input_seq.shape[0], 1), start_token, dtype=torch.long, device=self.device)\n",
    "            for _ in range(max_length):\n",
    "                target_embedded = self.output_embedding.forward(target_seq)\n",
    "                target_encoded = self.positional_encoding.forward(target_embedded)\n",
    "                decoder_output, _ = self.decoder.forward(target_encoded, encoded_output, training=False)\n",
    "                logits = torch.matmul(decoder_output[:, -1:], self.final_layer)\n",
    "                next_token = torch.argmax(logits, dim=-1)\n",
    "                target_seq = torch.cat([target_seq, next_token], dim=1)\n",
    "                if (next_token == end_token).all():\n",
    "                    break\n",
    "            return target_seq\n",
    "\n",
    "def cross_entropy_loss(predictions, targets, smoothing=0.1):\n",
    "    vocab_size = predictions.shape[-1]\n",
    "    confidence = 1.0 - smoothing\n",
    "    smoothing_labels = smoothing / vocab_size\n",
    "    \n",
    "    predictions = predictions.view(-1, vocab_size)\n",
    "    targets = targets.view(-1)\n",
    "    \n",
    "    one_hot = torch.zeros_like(predictions)\n",
    "    one_hot.scatter_(1, targets.unsqueeze(-1), 1)\n",
    "    smoothed_targets = one_hot * confidence + smoothing_labels\n",
    "    \n",
    "    log_probs = -torch.log(predictions + 1e-10)\n",
    "    loss = (smoothed_targets * log_probs).sum(dim=1).mean()\n",
    "    return loss\n",
    "\n",
    "def train(model, input_seq, target_seq, target_labels, epochs=50, warmup_steps=200):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    input_seq = input_seq.to(device)\n",
    "    target_seq = target_seq.to(device)\n",
    "    target_labels = target_labels.to(device)\n",
    "    \n",
    "    params = model.parameters()\n",
    "    optimizer = optim.Adam(params, lr=0.0005, betas=(0.9, 0.98), eps=1e-9)  # Higher base LR\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        output, _ = model.forward(input_seq, target_seq, training=True)\n",
    "        loss = cross_entropy_loss(output, target_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        step = epoch + 1\n",
    "        lr = (model.dmodel ** -0.5) * min(step ** -0.5, step * warmup_steps ** -1.5)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, LR: {lr:.6f}\")\n",
    "    return model\n",
    "\n",
    "# Small English-to-French translation demo\n",
    "if __name__ == \"__main__\":\n",
    "    # Hyperparameters (smaller for small data)\n",
    "    num_layers = 2\n",
    "    dmodel = 128  # Smaller embedding size\n",
    "    num_heads = 4\n",
    "    d_ff = 512    # Smaller FFN size\n",
    "    batch_size = 4\n",
    "    max_seq_length = 5\n",
    "    \n",
    "    # English and French combined for simplicity\n",
    "    vocab = {\"<pad>\": 0, \"<start>\": 1, \"<end>\": 2, \"I\": 3, \"like\": 4, \"to\": 5, \"read\": 6, \n",
    "             \"j\": 7, \"aime\": 8, \"lire\": 9}  # Small vocab\n",
    "    vocab_size = len(vocab)\n",
    "    idx_to_word = {i: w for w, i in vocab.items()}\n",
    "    \n",
    "    # Training data: English -> French (small batch)\n",
    "    # \"I like to read\" -> \"J'aime lire\"\n",
    "    input_seq = torch.tensor([\n",
    "        [3, 4, 5, 6, 0],  # I like to read <pad>\n",
    "        [3, 4, 5, 6, 0],\n",
    "        [3, 4, 5, 6, 0],\n",
    "        [3, 4, 5, 6, 0]\n",
    "    ], dtype=torch.long)  # Shape: [batch_size, seq_length]\n",
    "    \n",
    "    target_seq = torch.tensor([\n",
    "        [1, 7, 8, 9, 0],  # <start> j aime lire <pad>\n",
    "        [1, 7, 8, 9, 0],\n",
    "        [1, 7, 8, 9, 0],\n",
    "        [1, 7, 8, 9, 0]\n",
    "    ], dtype=torch.long)  # Decoder input\n",
    "    \n",
    "    target_labels = torch.tensor([\n",
    "        [7, 8, 9, 2, 0],  # j aime lire <end> <pad>\n",
    "        [7, 8, 9, 2, 0],\n",
    "        [7, 8, 9, 2, 0],\n",
    "        [7, 8, 9, 2, 0]\n",
    "    ], dtype=torch.long)  # Ground truth\n",
    "    \n",
    "    # Initialize and train\n",
    "    transformer = Transformer(num_layers, dmodel, num_heads, d_ff, vocab_size, vocab_size)\n",
    "    trained_model = train(transformer, input_seq, target_seq, target_labels, epochs=50)\n",
    "    \n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        translated = trained_model.translate(input_seq[:1], max_length=5)  # Translate first sentence\n",
    "        translated_words = [idx_to_word[idx.item()] for idx in translated[0]]\n",
    "        print(\"\\nInput: I like to read\")\n",
    "        print(\"Predicted translation:\", \" \".join(translated_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
