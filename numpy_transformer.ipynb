{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing word: I\n",
      "Original embedding: [1 0 0]\n",
      "\n",
      "For word 'I':\n",
      "Query vector (what I'm looking for): [1 0 0]\n",
      "Key vector (what I match against): [1 0 0]\n",
      "Value vector (what information I carry): [1 0 0]\n",
      "\n",
      "Attention score between 'I' and 'like': 0\n",
      "\n",
      "Visual Representation:\n",
      "Word 'I' looking for matches:\n",
      "┌─────────┐\n",
      "│   I    │ ──Query──> [1,0,0] • Compare with Keys of:\n",
      "└─────────┘\n",
      "                         │\n",
      "                         ├──> I     [1,0,0] = Score: 1.0\n",
      "                         ├──> like  [0,1,0] = Score: 0.0\n",
      "                         └──> Batman[0,0,1] = Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Let's use very small embeddings (3D) for visualization\n",
    "# Simplified word embeddings (normally these would be learned)\n",
    "word_embeddings = {\n",
    "    \"I\":      np.array([1, 0, 0]),\n",
    "    \"like\":   np.array([0, 1, 0]),\n",
    "    \"Batman\": np.array([0, 0, 1])\n",
    "}\n",
    "\n",
    "# Simplified weight matrices (normally these would be learned)\n",
    "W_query = np.array([\n",
    "    [1, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "\n",
    "W_key = np.array([\n",
    "    [1, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "\n",
    "W_value = np.array([\n",
    "    [1, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "\n",
    "# Let's look at what happens with the word \"I\"\n",
    "word = \"I\"\n",
    "embedding = word_embeddings[word]\n",
    "\n",
    "print(f\"Processing word: {word}\")\n",
    "print(f\"Original embedding: {embedding}\")\n",
    "\n",
    "# Create Q, K, V for this word\n",
    "Q = np.dot(embedding, W_query)\n",
    "K = np.dot(embedding, W_key)\n",
    "V = np.dot(embedding, W_value)\n",
    "\n",
    "print(\"\\nFor word 'I':\")\n",
    "print(f\"Query vector (what I'm looking for): {Q}\")\n",
    "print(f\"Key vector (what I match against): {K}\")\n",
    "print(f\"Value vector (what information I carry): {V}\")\n",
    "\n",
    "# Now let's see how \"I\" interacts with \"like\"\n",
    "other_word = \"like\"\n",
    "other_K = np.dot(word_embeddings[other_word], W_key)\n",
    "\n",
    "# Calculate attention score between \"I\" and \"like\"\n",
    "attention_score = np.dot(Q, other_K)\n",
    "print(f\"\\nAttention score between '{word}' and '{other_word}': {attention_score}\")\n",
    "\n",
    "# Visual representation\n",
    "print(\"\\nVisual Representation:\")\n",
    "print(\"Word 'I' looking for matches:\")\n",
    "print(\"┌─────────┐\")\n",
    "print(\"│   I    │ ──Query──> [1,0,0] • Compare with Keys of:\")\n",
    "print(\"└─────────┘\")\n",
    "print(\"                         │\")\n",
    "print(\"                         ├──> I     [1,0,0] = Score: 1.0\")\n",
    "print(\"                         ├──> like  [0,1,0] = Score: 0.0\")\n",
    "print(\"                         └──> Batman[0,0,1] = Score: 0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attention weights for 'I like Batman':\n",
      "I -> I: 0.203\n",
      "I -> like: 0.498\n",
      "I -> Batman: 0.298\n",
      "like -> I: 0.511\n",
      "like -> like: 0.363\n",
      "like -> Batman: 0.126\n",
      "Batman -> I: 0.097\n",
      "Batman -> like: 0.322\n",
      "Batman -> Batman: 0.581\n",
      "\n",
      "Attention weights for 'I like Dwight Schrute from The Office':\n",
      "I -> I: 0.071\n",
      "I -> like: 0.174\n",
      "I -> Dwight: 0.077\n",
      "I -> Schrute: 0.280\n",
      "I -> from: 0.043\n",
      "I -> The: 0.103\n",
      "I -> Office: 0.252\n",
      "like -> I: 0.090\n",
      "like -> like: 0.064\n",
      "like -> Dwight: 0.153\n",
      "like -> Schrute: 0.219\n",
      "like -> from: 0.259\n",
      "like -> The: 0.086\n",
      "like -> Office: 0.130\n",
      "Dwight -> I: 0.166\n",
      "Dwight -> like: 0.082\n",
      "Dwight -> Dwight: 0.139\n",
      "Dwight -> Schrute: 0.219\n",
      "Dwight -> from: 0.120\n",
      "Dwight -> The: 0.161\n",
      "Dwight -> Office: 0.113\n",
      "Schrute -> I: 0.092\n",
      "Schrute -> like: 0.197\n",
      "Schrute -> Dwight: 0.264\n",
      "Schrute -> Schrute: 0.059\n",
      "Schrute -> from: 0.330\n",
      "Schrute -> The: 0.025\n",
      "Schrute -> Office: 0.032\n",
      "from -> I: 0.314\n",
      "from -> like: 0.087\n",
      "from -> Dwight: 0.096\n",
      "from -> Schrute: 0.254\n",
      "from -> from: 0.140\n",
      "from -> The: 0.035\n",
      "from -> Office: 0.073\n",
      "The -> I: 0.105\n",
      "The -> like: 0.074\n",
      "The -> Dwight: 0.357\n",
      "The -> Schrute: 0.026\n",
      "The -> from: 0.274\n",
      "The -> The: 0.110\n",
      "The -> Office: 0.054\n",
      "Office -> I: 0.128\n",
      "Office -> like: 0.228\n",
      "Office -> Dwight: 0.130\n",
      "Office -> Schrute: 0.120\n",
      "Office -> from: 0.083\n",
      "Office -> The: 0.159\n",
      "Office -> Office: 0.151\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SelfAttention:\n",
    "    def __init__(self, embedding_dim):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.W_query = np.random.randn(embedding_dim, embedding_dim)\n",
    "        self.W_key = np.random.randn(embedding_dim, embedding_dim)\n",
    "        self.W_value = np.random.randn(embedding_dim, embedding_dim)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Q = np.dot(X, self.W_query)\n",
    "        K = np.dot(X, self.W_key)\n",
    "        V = np.dot(X, self.W_value)\n",
    "        \n",
    "        attention_scores = np.dot(Q, K.T)\n",
    "        attention_scores = attention_scores / np.sqrt(self.embedding_dim)\n",
    "        attention_weights = self.softmax(attention_scores)\n",
    "        \n",
    "        output = np.dot(attention_weights, V)\n",
    "        return output, attention_weights\n",
    "\n",
    "# Process sentences\n",
    "sentence1 = [\"I\", \"like\", \"Batman\"]\n",
    "sentence2 = [\"I\", \"like\", \"Dwight\", \"Schrute\", \"from\", \"The\", \"Office\"]\n",
    "\n",
    "# Combine unique words from both sentences\n",
    "unique_words = list(set(sentence1 + sentence2))\n",
    "vocab_size = len(unique_words)\n",
    "word_to_idx = {word: idx for idx, word in enumerate(unique_words)}\n",
    "\n",
    "# Create one-hot encodings\n",
    "def create_one_hot(sentence, word_to_idx, vocab_size):\n",
    "    encodings = np.zeros((len(sentence), vocab_size))\n",
    "    for i, word in enumerate(sentence):\n",
    "        encodings[i][word_to_idx[word]] = 1\n",
    "    return encodings\n",
    "\n",
    "# Process each sentence separately\n",
    "X1 = create_one_hot(sentence1, word_to_idx, vocab_size)\n",
    "X2 = create_one_hot(sentence2, word_to_idx, vocab_size)\n",
    "\n",
    "# Initialize and apply self-attention\n",
    "attention = SelfAttention(embedding_dim=vocab_size)\n",
    "\n",
    "# Process sentence 1\n",
    "output1, attention_weights1 = attention.forward(X1)\n",
    "print(\"\\nAttention weights for 'I like Batman':\")\n",
    "for i, word1 in enumerate(sentence1):\n",
    "    for j, word2 in enumerate(sentence1):\n",
    "        print(f\"{word1} -> {word2}: {attention_weights1[i][j]:.3f}\")\n",
    "\n",
    "# Process sentence 2\n",
    "output2, attention_weights2 = attention.forward(X2)\n",
    "print(\"\\nAttention weights for 'I like Dwight Schrute from The Office':\")\n",
    "for i, word1 in enumerate(sentence2):\n",
    "    for j, word2 in enumerate(sentence2):\n",
    "        print(f\"{word1} -> {word2}: {attention_weights2[i][j]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1.14853278,  0.24294323, -0.77837852,  0.46768282,\n",
       "          1.10207346, -1.37347654],\n",
       "        [-0.33810614,  0.67845663, -1.38402979, -1.17511333,\n",
       "          0.78840684,  0.20958856],\n",
       "        [-1.00998464,  0.95613032, -1.40618625,  0.77488933,\n",
       "         -0.02233755, -1.59799296],\n",
       "        [ 1.18096428,  0.67515626, -0.73140025,  0.57697706,\n",
       "          0.91110612,  0.07475426],\n",
       "        [ 0.02173382,  2.14251567, -2.75189226, -1.16521825,\n",
       "          0.65411177, -0.18243588],\n",
       "        [-0.45791856, -1.55752755,  0.17662621, -0.1791015 ,\n",
       "          1.21998309,  1.12747096]],\n",
       "\n",
       "       [[-0.69879681,  0.85213555, -0.15920229, -1.70878087,\n",
       "         -2.71404885, -1.87139264],\n",
       "        [-0.783748  ,  0.30179782,  0.35268233, -0.29266392,\n",
       "          0.50183542, -2.12717599],\n",
       "        [-0.45232925, -0.49042193, -0.9220487 ,  0.07075109,\n",
       "          0.40954338, -1.03936272],\n",
       "        [-0.12082068,  1.88814436,  0.15811987,  0.77821047,\n",
       "         -0.09576802, -0.07125079],\n",
       "        [-1.08351856,  1.05159847, -0.11717138, -0.80759915,\n",
       "         -0.34856653, -0.9546943 ],\n",
       "        [ 1.3853512 , -0.01527413,  0.162903  , -1.91523873,\n",
       "         -0.25617202,  1.86924469]],\n",
       "\n",
       "       [[ 0.66930247, -1.38611279, -0.83166372,  0.85114462,\n",
       "         -0.55257223, -0.36282721],\n",
       "        [ 0.24619787,  1.04741661,  0.33507707,  0.65788078,\n",
       "          0.61387991,  2.18589565],\n",
       "        [-0.95317417, -1.62969983, -0.77617573, -1.25428746,\n",
       "         -0.03156712, -0.16383474],\n",
       "        [-0.57638054,  1.22186565, -0.85610285,  0.65587416,\n",
       "          0.23364709, -0.79469552],\n",
       "        [ 0.7824887 ,  1.34404682, -0.29463324,  0.90241136,\n",
       "         -1.14456403,  0.28558736],\n",
       "        [ 0.89173   ,  0.05726844,  0.17478293, -0.33631374,\n",
       "         -0.59879331, -1.3239338 ]]])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multihead_W_query = np.random.randn(3, 6, 6)\n",
    "multihead_W_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, p=0.1):\n",
    "        self.p = p\n",
    "    def forward(self, x, training=True):\n",
    "        if not training or self.p == 0:\n",
    "            return x\n",
    "        \n",
    "        mask = np.random.binomial(1, 1-self.p, x.shape) / (1-self.p)\n",
    "        return x * mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention:\n",
    "    def __init__(self, embedding_dim, num_head, dropout=0.1):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_head = num_head\n",
    "\n",
    "        self.head_dim = embedding_dim // num_head\n",
    "\n",
    "        self.W_query = [np.random.randn(embedding_dim, self.head_dim) for _ in range(num_head)]\n",
    "        self.W_key = [np.random.randn(embedding_dim, self.head_dim) for _ in range(num_head)]\n",
    "        self.W_value = [np.random.randn(embedding_dim, self.head_dim) for _ in range(num_head)]\n",
    "\n",
    "        total_head_dim = self.head_dim * num_head\n",
    "        self.W_output = np.random.randn(total_head_dim, embedding_dim)\n",
    "\n",
    "        self.dropout = Dropout(dropout)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "    \n",
    "    def attention(self, Q, K, V, mask=None):\n",
    "        batch_size = Q.shape[0]\n",
    "        \n",
    "        # Reshape for batch matrix multiplication\n",
    "        Q_flat = Q.reshape(-1, Q.shape[-1])\n",
    "        K_flat = K.reshape(-1, K.shape[-1])\n",
    "        V_flat = V.reshape(-1, V.shape[-1])\n",
    "\n",
    "        scores = np.dot(Q_flat, K_flat.T)\n",
    "        scores = scores / np.sqrt(self.head_dim)\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores * mask + -1e9 * (1 - mask)\n",
    "        \n",
    "        weights = self.softmax(scores)\n",
    "        return np.dot(weights, V_flat), weights\n",
    "    \n",
    "    def forward(self, X, memory = None, mask=None,training=True):\n",
    "        head_outputs = []\n",
    "        all_attention_weights = []\n",
    "\n",
    "        for head in range(self.num_head):\n",
    "            Q = np.dot(X, self.W_query[head])\n",
    "\n",
    "            if memory is not None:\n",
    "                K = np.dot(memory, self.W_key[head])\n",
    "                V = np.dot(memory, self.W_value[head])\n",
    "            else:\n",
    "                K = np.dot(X, self.W_key[head])\n",
    "                V = np.dot(X, self.W_value[head])\n",
    "\n",
    "            head_output, attention_weights = self.attention(Q, K, V)\n",
    "            dropout = self.dropout.forward(head_output, training)\n",
    "            head_outputs.append(dropout)\n",
    "            all_attention_weights.append(attention_weights)\n",
    "        \n",
    "        multi_head_output = np.concatenate(head_outputs, axis = -1)\n",
    "\n",
    "        final_output = np.dot(multi_head_output, self.W_output)\n",
    "\n",
    "        final_output = self.dropout.forward(final_output, training)\n",
    "\n",
    "        return final_output, all_attention_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feed Forward Network\n",
    "class FFN():\n",
    "    def __init__(self, dmodel, d_ff): #dmodel = 512 & d_ff = 2048 as per paper\n",
    "        self.W1 = np.random.randn(dmodel, d_ff)\n",
    "        self.W2 = np.random.randn(d_ff, dmodel)\n",
    "        self.b1 = np.random.randn(d_ff)\n",
    "        self.b2 = np.random.randn(dmodel)\n",
    "\n",
    "    def reLU(self, X):\n",
    "        return np.maximum(0, X)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.Z1 = np.dot(X,self.W1) + self.b1\n",
    "        self.A1 = self.reLU(self.Z1)\n",
    "        self.Z2 = np.dot(self.A1,self.W2) + self.b2\n",
    "        return self.Z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding:\n",
    "    def __init__(self, dmodel,max_seq_length):\n",
    "        self.encoding = np.zeros((max_seq_length, dmodel))\n",
    "        for pos in range(max_seq_length):\n",
    "            for j in range(int(dmodel/2)):\n",
    "                 denominator = 10000**(2*j/dmodel)\n",
    "                 self.encoding[pos, 2*j] = np.sin(pos/denominator)\n",
    "                 self.encoding[pos, 2*j+1] = np.cos((pos/denominator))\n",
    "    \n",
    "    def forward(self,X):\n",
    "        seq_length = X.shape[1]\n",
    "        positions_needed = self.encoding[:seq_length]\n",
    "        return X + positions_needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, vocab_size, dmodel):\n",
    "        self.embedding_matrix = np.random.randn(vocab_size, dmodel)\n",
    "        self.dmodel = dmodel\n",
    "        pass\n",
    "    def forward(self,x):\n",
    "        return self.embedding_matrix[x] * np.sqrt(self.dmodel) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization:\n",
    "    def __init__(self, dmodel, epsilon = 1e-12):\n",
    "        self.gamma = np.ones(dmodel)\n",
    "        self.beta = np.zeros(dmodel)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = np.mean(x, axis=-1, keepdims=True)\n",
    "        variance = np.var(x, axis=-1, keepdims=True)\n",
    "        normalize = (x - mean) / np.sqrt(variance + self.epsilon)\n",
    "        scale_shift = self.gamma * normalize + self.beta\n",
    "        return scale_shift\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer:\n",
    "    def __init__(self, dmodel, num_heads, d_ff, dropout=0.1):\n",
    "        self.mha = MultiHeadAttention(dmodel, num_heads)\n",
    "\n",
    "        self.ffn = FFN(dmodel,d_ff)\n",
    "\n",
    "        self.norm1 = LayerNormalization(dmodel) #after mha\n",
    "        self.norm2 = LayerNormalization(dmodel) #after ffn\n",
    "        self.dropout = Dropout(dropout)\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        atten_output, atten_weights = self.mha.forward(X)\n",
    "        dropout1 = self.dropout.forward(atten_output, training)\n",
    "        output1 = self.norm1.forward(X + dropout1)\n",
    "\n",
    "        ffn_output = self.ffn.forward(output1)\n",
    "        dropout2 = self.dropout.forward(ffn_output, training)\n",
    "        output2 = self.norm2.forward(output1 + dropout2)\n",
    "\n",
    "        return output2, atten_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    def __init__(self, num_layers, dmodel, num_heads, d_ff):\n",
    "        self.layers = [EncoderLayer(dmodel,num_heads,d_ff) for _ in range(num_layers)]\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        attention_weights = []\n",
    "        for layer in self.layers:\n",
    "            outs, weights = layer.forward(X)\n",
    "            attention_weights.append(weights)\n",
    "        \n",
    "        return outs, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer:\n",
    "    def __init__(self, dmodel, num_heads, d_ff, dropout=0.1):\n",
    "        self.masked_mha = MultiHeadAttention(dmodel, num_heads)\n",
    "\n",
    "        self.mha = MultiHeadAttention(dmodel, num_heads)\n",
    "\n",
    "        self.ffn = FFN(dmodel, d_ff)\n",
    "\n",
    "        self.norm1 = LayerNormalization(dmodel)\n",
    "        self.norm2 = LayerNormalization(dmodel)\n",
    "        self.norm3 = LayerNormalization(dmodel)\n",
    "\n",
    "        self.dropout = Dropout(dropout)\n",
    "    \n",
    "    def create_mask(self, size):\n",
    "        mask = np.tril(np.ones((size,size)))\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, X, encoded_output, training=True):\n",
    "        seq_length = X.shape[1]\n",
    "        mask = self.create_mask(seq_length)\n",
    "\n",
    "        masked_atten_output, masked_atten_weights = self.masked_mha.forward(X, memory=None, mask=mask)\n",
    "        dropout1 = self.dropout.forward(masked_atten_output, training)\n",
    "        out1 = self.norm1.forward(X + dropout1)\n",
    "\n",
    "        atten_output, atten_weights = self.mha.forward(out1, memory=encoded_output)\n",
    "        dropout2 = self.dropout.forward(atten_output, training)\n",
    "        out2 = self.norm2.forward(out1 + dropout2)\n",
    "\n",
    "        ffn_output = self.ffn.forward(out2)\n",
    "        dropout3 = self.dropout.forward(ffn_output, training)\n",
    "        out3 = self.norm3.forward(out2 + dropout3)\n",
    "\n",
    "        return out3, (masked_atten_weights, atten_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder:\n",
    "    def __init__(self, num_layers, dmodel, num_heads, d_ff):\n",
    "        self.layers = [DecoderLayer(dmodel, num_heads, d_ff) for _ in range(num_layers)]\n",
    "    \n",
    "    def forward(self, X, encoded_output, training=True):\n",
    "        attention_weights = []\n",
    "        for layer in self.layers:\n",
    "            outs, weights = layer.forward(X, encoded_output)\n",
    "            attention_weights.append(weights)\n",
    "        \n",
    "        return outs, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence shape: (1, 3)\n",
      "Target sequence shape: (1, 3)\n",
      "Output shape: (1, 3, 1000)\n"
     ]
    }
   ],
   "source": [
    "class Transformer:\n",
    "    def __init__(self, num_layers, dmodel, num_heads, d_ff, input_vocab_size, target_vocab_size, dropout=0.1):\n",
    "        self.dmodel = dmodel\n",
    "        #Embedding\n",
    "        self.input_embedding = Embedding(input_vocab_size, dmodel)\n",
    "        self.output_embedding = Embedding(target_vocab_size, dmodel)\n",
    "\n",
    "        #Positional Encoding\n",
    "        self.positional_encoding = PositionalEncoding(dmodel, max_seq_length = 5000)\n",
    "\n",
    "        #Dropout\n",
    "        self.dropout = Dropout(dropout)\n",
    "\n",
    "        #Encoder and Decoder\n",
    "        self.encoder = Encoder(num_layers, dmodel, num_heads, d_ff)\n",
    "        self.decoder = Decoder(num_layers, dmodel, num_heads, d_ff)\n",
    "\n",
    "        #Final linear layer and softmax\n",
    "        self.final_layer = np.random.randn(dmodel, target_vocab_size)\n",
    "\n",
    "    def forward(self, input_seq, target_seq, training=True):\n",
    "        #Input embedding + Positional Encoding\n",
    "        input_embedded = self.input_embedding.forward(input_seq)\n",
    "        input_encoded = self.positional_encoding.forward(input_embedded)\n",
    "        input_dropped = self.dropout.forward(input_encoded, training)\n",
    "\n",
    "\n",
    "        #Encoder\n",
    "        encoded_output, encoder_attention = self.encoder.forward(input_dropped, training)\n",
    "\n",
    "        #Target embedding + Positional Encoding\n",
    "        target_embedded = self.output_embedding.forward(target_seq)\n",
    "        target_encoded = self.positional_encoding.forward(target_embedded)\n",
    "        target_dropped = self.dropout.forward(target_encoded, training)\n",
    "\n",
    "        #Decoder\n",
    "        decoder_output, decoder_attention = self.decoder.forward(target_dropped, encoded_output, training)\n",
    "\n",
    "        #Final linear layer and softmax\n",
    "        logits = np.dot(decoder_output, self.final_layer)\n",
    "        probs = self.softmax(logits)\n",
    "        \n",
    "        return probs, (encoder_attention, decoder_attention)\n",
    "\n",
    "    def softmax(self,x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "# Let's test it\n",
    "if __name__ == \"__main__\":\n",
    "    # Test parameters (smaller for testing)\n",
    "    num_layers = 2\n",
    "    d_model = 6\n",
    "    num_heads = 2\n",
    "    d_ff = 8\n",
    "    input_vocab_size = 1000\n",
    "    target_vocab_size = 1000\n",
    "    \n",
    "    # Create transformer\n",
    "    transformer = Transformer(num_layers, d_model, num_heads, d_ff, \n",
    "                           input_vocab_size, target_vocab_size)\n",
    "    \n",
    "    # Create sample inputs\n",
    "    batch_size = 1\n",
    "    seq_length = 3\n",
    "    input_seq = np.random.randint(0, input_vocab_size, (batch_size, seq_length))\n",
    "    target_seq = np.random.randint(0, target_vocab_size, (batch_size, seq_length))\n",
    "    \n",
    "    # Forward pass\n",
    "    output, attention = transformer.forward(input_seq, target_seq)\n",
    "    \n",
    "    print(\"Input sequence shape:\", input_seq.shape)\n",
    "    print(\"Target sequence shape:\", target_seq.shape)\n",
    "    print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 1000: 0.001398\n",
      "\n",
      "Shapes:\n",
      "Predictions: (2, 3, 5)\n",
      "Targets: (2, 3)\n",
      "\n",
      "Loss value: 1.8229\n"
     ]
    }
   ],
   "source": [
    "#Traning\n",
    "def softmax(x, axis=-1):\n",
    "    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "#learning rate\n",
    "def get_learning_rate(step, dmodel, warmup_steps=4000):\n",
    "    return dmodel ** (-0.5) * min(step ** (-0.5), step * warmup_steps ** (-0.5))\n",
    "\n",
    "#Loss function\n",
    "def cross_entropy_loss(predictions, targets, smoothing=0.1):\n",
    "    vocab_size = predictions.shape[-1]\n",
    "\n",
    "    confidence = 1.0 - smoothing\n",
    "    smoothing_labels = smoothing / vocab_size\n",
    "\n",
    "\n",
    "    batch_size, seq_length, vocab_size = predictions.shape\n",
    "    predictions = predictions.reshape(-1, vocab_size)\n",
    "    targets = targets.reshape(-1)\n",
    "\n",
    "    one_hot = np.zeros_like(predictions)\n",
    "    one_hot[np.arange(len(targets)), targets] = 1\n",
    "    smoothed_targets = one_hot * confidence + smoothing_labels\n",
    "\n",
    "    log_probs = -np.log(predictions + 1e-10)\n",
    "    loss = np.mean(np.sum(smoothed_targets * log_probs, axis=1))\n",
    "     \n",
    "    return loss\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test learning rate\n",
    "    step = 1000\n",
    "    d_model = 512\n",
    "    lr = get_learning_rate(step, d_model)\n",
    "    print(f\"Learning rate at step {step}: {lr:.6f}\")\n",
    "    \n",
    "    # Test loss function\n",
    "    batch_size = 2\n",
    "    seq_length = 3\n",
    "    vocab_size = 5\n",
    "    \n",
    "    # Create test data\n",
    "    logits = np.random.randn(batch_size, seq_length, vocab_size)\n",
    "    predictions = softmax(logits, axis=-1)\n",
    "    targets = np.random.randint(0, vocab_size, (batch_size, seq_length))\n",
    "    \n",
    "    # Print shapes for debugging\n",
    "    print(\"\\nShapes:\")\n",
    "    print(\"Predictions:\", predictions.shape)\n",
    "    print(\"Targets:\", targets.shape)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = cross_entropy_loss(predictions, targets)\n",
    "    print(f\"\\nLoss value: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
