# Transformer-from-Scratch-NumPy-to-PyTorch
Implemented the Transformer model from scratch using NumPy, replicating the "Attention Is All You Need" architecture. Later, I ported it to PyTorch for efficient GPU-accelerated training. This project was done as a learning challenge to deeply understand self-attention, multi-head attention, and the inner workings of Transformers.
